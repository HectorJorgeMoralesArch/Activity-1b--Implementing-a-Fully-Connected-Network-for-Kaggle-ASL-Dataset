{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TC 5033\n",
    "## Deep Learning\n",
    "## Fully Connected Deep Neural Networks\n",
    "\n",
    "#### Activity 1b: Implementing a Fully Connected Network for Kaggle ASL Dataset\n",
    "\n",
    "- Objective\n",
    "\n",
    "The aim of this part of the activity is to apply your understanding of Fully Connected Networks by implementing a multilayer network for the [Kaggle ASL (American Sign Language) dataset](https://www.kaggle.com/datasets/grassknoted/asl-alphabet). While you have been provided with a complete solution for a Fully Connected Network using Numpy for the MNIST dataset, you are encouraged to try to come up with the solution.\n",
    "\n",
    "- Instructions\n",
    "\n",
    "    This activity requires submission in teams of 3 or 4 members. Submissions from smaller or larger teams will not be accepted unless prior approval has been granted (only due to exceptional circumstances). While teamwork is encouraged, each member is expected to contribute individually to the assignment. The final submission should feature the best arguments and solutions from each team member. Only one person per team needs to submit the completed work, but it is imperative that the names of all team members are listed in a Markdown cell at the very beginning of the notebook (either the first or second cell). Failure to include all team member names will result in the grade being awarded solely to the individual who submitted the assignment, with zero points given to other team members (no exceptions will be made to this rule).\n",
    "\n",
    "    Load and Preprocess Data: You are provided a starter code to load the data. Be sure to understand the code.\n",
    "\n",
    "    Review MNIST Notebook (Optional): Before diving into this activity, you have the option to revisit the MNIST example to refresh your understanding of how to build a Fully Connected Network using Numpy.\n",
    "\n",
    "    Start Fresh: Although you can refer to the MNIST solution at any point, try to implement the network for the ASL dataset on your own. This will reinforce your learning and understanding of the architecture and mathematics involved.\n",
    "\n",
    "    Implement Forward and Backward Pass: Write the code to perform the forward and backward passes, keeping in mind the specific challenges and characteristics of the ASL dataset.\n",
    "    \n",
    "     Design the Network: Create the architecture of the Fully Connected Network tailored for the ASL dataset. Choose the number of hidden layers, neurons, and hyperparameters judiciously.\n",
    "\n",
    "    Train the Model: Execute the training loop, ensuring to track performance metrics such as loss and accuracy.\n",
    "\n",
    "    Analyze and Document: Use Markdown cells to document in detail the choices you made in terms of architecture and hyperparameters, you may use figures, equations, etc to aid in your explanations. Include any metrics that help justify these choices and discuss the model's performance.  \n",
    "\n",
    "- Evaluation Criteria\n",
    "\n",
    "    - Code Readability and Comments\n",
    "    - Appropriateness of chosen architecture and hyperparameters for the ASL dataset\n",
    "    - Performance of the model on the ASL dataset (at least 70% acc)\n",
    "    - Quality of Markdown documentation\n",
    "\n",
    "- Submission\n",
    "\n",
    "Submit this Jupyter Notebook in canvas with your complete solution, ensuring your code is well-commented and includes Markdown cells that explain your design choices, results, and any challenges you encountered.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of numpy.core failed: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 455, in superreload\n",
      "    if not append_obj(module, old_objects, name, obj):\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 423, in append_obj\n",
      "    in_module = hasattr(obj, \"__module__\") and obj.__module__ == module.__name__\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\core\\defchararray.py\", line 2, in __getattr__\n",
      "    from numpy._core import defchararray\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\_core\\defchararray.py\", line 23, in <module>\n",
      "    from numpy._core.strings import (\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\_core\\strings.py\", line 93, in <module>\n",
      "    MAX = np.iinfo(np.int64).max\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\core\\getlimits.py\", line 687, in __init__\n",
      "NameError: name 'numeric' is not defined\n",
      "]\n",
      "[autoreload of numpy failed: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 455, in superreload\n",
      "    if not append_obj(module, old_objects, name, obj):\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 423, in append_obj\n",
      "    in_module = hasattr(obj, \"__module__\") and obj.__module__ == module.__name__\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\core\\defchararray.py\", line 2, in __getattr__\n",
      "    from numpy._core import defchararray\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\_core\\defchararray.py\", line 23, in <module>\n",
      "    from numpy._core.strings import (\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\_core\\strings.py\", line 93, in <module>\n",
      "    MAX = np.iinfo(np.int64).max\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\core\\getlimits.py\", line 687, in __init__\n",
      "NameError: name 'numeric' is not defined\n",
      "]\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "contourpy 1.2.0 requires numpy<2.0,>=1.20, but you have numpy 2.4.2 which is incompatible.\n",
      "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.4.2 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.4.2 which is incompatible.\n",
      "scipy 1.13.1 requires numpy<2.3,>=1.22.4, but you have numpy 2.4.2 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Using cached numpy-2.4.2-cp312-cp312-win_amd64.whl.metadata (6.6 kB)\n",
      "Using cached numpy-2.4.2-cp312-cp312-win_amd64.whl (12.3 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.4.2\n",
      "    Uninstalling numpy-2.4.2:\n",
      "      Successfully uninstalled numpy-2.4.2\n",
      "Successfully installed numpy-2.4.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of numpy._core._add_newdocs failed: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\importlib\\__init__.py\", line 131, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 866, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 995, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\_core\\_add_newdocs.py\", line 7084, in <module>\n",
      "    add_newdoc('numpy.dtypes', _dtype_name,\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\_core\\function_base.py\", line 537, in add_newdoc\n",
      "    new = getattr(__import__(place, globals(), {}, [obj]), obj)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: module 'numpy.dtypes' has no attribute 'BoolDType'\n",
      "]\n",
      "c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\ma\\core.py:86: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
      "  MaskType = np.bool\n",
      "[autoreload of numpy.ma.core failed: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\importlib\\__init__.py\", line 131, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 866, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 995, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\ma\\core.py\", line 86, in <module>\n",
      "    MaskType = np.bool\n",
      "               ^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\__init__.py\", line 324, in __getattr__\n",
      "    linspace,\n",
      "        ^^^^^^\n",
      "AttributeError: module 'numpy' has no attribute 'bool'.\n",
      "`np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n",
      "    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations. Did you mean: 'bool_'?\n",
      "]\n",
      "c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\_typing\\_scalars.py:11: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
      "  _BoolLike_co: TypeAlias = bool | np.bool\n",
      "[autoreload of numpy._typing._scalars failed: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\importlib\\__init__.py\", line 131, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 866, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 995, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\_typing\\_scalars.py\", line 11, in <module>\n",
      "    _BoolLike_co: TypeAlias = bool | np.bool\n",
      "                                     ^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\__init__.py\", line 324, in __getattr__\n",
      "    linspace,\n",
      "        ^^^^^^\n",
      "AttributeError: module 'numpy' has no attribute 'bool'.\n",
      "`np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n",
      "    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations. Did you mean: 'bool_'?\n",
      "]\n",
      "c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\_typing\\_dtype_like.py:83: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
      "  _DTypeLikeBool: TypeAlias = type[bool] | _DTypeLike[np.bool] | _BoolCodes\n",
      "[autoreload of numpy._typing._dtype_like failed: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\importlib\\__init__.py\", line 131, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 866, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 995, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\_typing\\_dtype_like.py\", line 83, in <module>\n",
      "    _DTypeLikeBool: TypeAlias = type[bool] | _DTypeLike[np.bool] | _BoolCodes\n",
      "                                                        ^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\__init__.py\", line 324, in __getattr__\n",
      "    linspace,\n",
      "        ^^^^^^\n",
      "AttributeError: module 'numpy' has no attribute 'bool'.\n",
      "`np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n",
      "    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations. Did you mean: 'bool_'?\n",
      "]\n",
      "c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\_typing\\_array_like.py:84: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
      "  _ArrayLikeBool_co: TypeAlias = _DualArrayLike[dtype[np.bool], bool]\n",
      "[autoreload of numpy._typing._array_like failed: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\importlib\\__init__.py\", line 131, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 866, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 995, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\_typing\\_array_like.py\", line 84, in <module>\n",
      "    _ArrayLikeBool_co: TypeAlias = _DualArrayLike[dtype[np.bool], bool]\n",
      "                                                        ^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\__init__.py\", line 324, in __getattr__\n",
      "    linspace,\n",
      "        ^^^^^^\n",
      "AttributeError: module 'numpy' has no attribute 'bool'.\n",
      "`np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n",
      "    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations. Did you mean: 'bool_'?\n",
      "]\n",
      "[autoreload of numpy._typing failed: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\importlib\\__init__.py\", line 131, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 866, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 995, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\_typing\\__init__.py\", line 5, in <module>\n",
      "    from ._array_like import (\n",
      "ImportError: cannot import name '_ArrayLikeAnyString_co' from 'numpy._typing._array_like' (c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\_typing\\_array_like.py)\n",
      "]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\hjma9\\anaconda3\\envs\\env_pyspark\\lib\\site-packages (4.13.0.90)\n",
      "Requirement already satisfied: numpy>=2 in c:\\users\\hjma9\\anaconda3\\envs\\env_pyspark\\lib\\site-packages (from opencv-python) (2.4.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Using cached numpy-2.4.2-cp312-cp312-win_amd64.whl.metadata (6.6 kB)\n",
      "Using cached numpy-2.4.2-cp312-cp312-win_amd64.whl (12.3 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.4.2\n",
      "    Uninstalling numpy-2.4.2:\n",
      "      Successfully uninstalled numpy-2.4.2\n",
      "Successfully installed numpy-2.4.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "contourpy 1.2.0 requires numpy<2.0,>=1.20, but you have numpy 2.4.2 which is incompatible.\n",
      "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.4.2 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.4.2 which is incompatible.\n",
      "scipy 1.13.1 requires numpy<2.3,>=1.22.4, but you have numpy 2.4.2 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --force-reinstall numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of numpy.core failed: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 455, in superreload\n",
      "    if not append_obj(module, old_objects, name, obj):\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 423, in append_obj\n",
      "    in_module = hasattr(obj, \"__module__\") and obj.__module__ == module.__name__\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\core\\defchararray.py\", line 2, in __getattr__\n",
      "    from numpy._core import defchararray\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\_core\\defchararray.py\", line 23, in <module>\n",
      "    from numpy._core.strings import (\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\_core\\strings.py\", line 93, in <module>\n",
      "    MAX = np.iinfo(np.int64).max\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\core\\getlimits.py\", line 687, in __init__\n",
      "NameError: name 'numeric' is not defined\n",
      "]\n",
      "[autoreload of numpy failed: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 455, in superreload\n",
      "    if not append_obj(module, old_objects, name, obj):\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 423, in append_obj\n",
      "    in_module = hasattr(obj, \"__module__\") and obj.__module__ == module.__name__\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\core\\defchararray.py\", line 2, in __getattr__\n",
      "    from numpy._core import defchararray\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\_core\\defchararray.py\", line 23, in <module>\n",
      "    from numpy._core.strings import (\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\_core\\strings.py\", line 93, in <module>\n",
      "    MAX = np.iinfo(np.int64).max\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\core\\getlimits.py\", line 687, in __init__\n",
      "NameError: name 'numeric' is not defined\n",
      "]\n",
      "[autoreload of numpy._core._add_newdocs failed: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\importlib\\__init__.py\", line 131, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 866, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 995, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\_core\\_add_newdocs.py\", line 7084, in <module>\n",
      "    add_newdoc('numpy.dtypes', _dtype_name,\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\_core\\function_base.py\", line 537, in add_newdoc\n",
      "    new = getattr(__import__(place, globals(), {}, [obj]), obj)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: module 'numpy.dtypes' has no attribute 'BoolDType'\n",
      "]\n",
      "c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\ma\\core.py:86: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
      "  MaskType = np.bool\n",
      "[autoreload of numpy.ma.core failed: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\importlib\\__init__.py\", line 131, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 866, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 995, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\ma\\core.py\", line 86, in <module>\n",
      "    MaskType = np.bool\n",
      "               ^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\__init__.py\", line 324, in __getattr__\n",
      "    linspace,\n",
      "        ^^^^^^\n",
      "AttributeError: module 'numpy' has no attribute 'bool'.\n",
      "`np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n",
      "    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations. Did you mean: 'bool_'?\n",
      "]\n",
      "c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\_typing\\_scalars.py:11: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
      "  _BoolLike_co: TypeAlias = bool | np.bool\n",
      "[autoreload of numpy._typing._scalars failed: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\importlib\\__init__.py\", line 131, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 866, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 995, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\_typing\\_scalars.py\", line 11, in <module>\n",
      "    _BoolLike_co: TypeAlias = bool | np.bool\n",
      "                                     ^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\__init__.py\", line 324, in __getattr__\n",
      "    linspace,\n",
      "        ^^^^^^\n",
      "AttributeError: module 'numpy' has no attribute 'bool'.\n",
      "`np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n",
      "    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations. Did you mean: 'bool_'?\n",
      "]\n",
      "c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\_typing\\_dtype_like.py:83: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
      "  _DTypeLikeBool: TypeAlias = type[bool] | _DTypeLike[np.bool] | _BoolCodes\n",
      "[autoreload of numpy._typing._dtype_like failed: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\importlib\\__init__.py\", line 131, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 866, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 995, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\_typing\\_dtype_like.py\", line 83, in <module>\n",
      "    _DTypeLikeBool: TypeAlias = type[bool] | _DTypeLike[np.bool] | _BoolCodes\n",
      "                                                        ^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\__init__.py\", line 324, in __getattr__\n",
      "    linspace,\n",
      "        ^^^^^^\n",
      "AttributeError: module 'numpy' has no attribute 'bool'.\n",
      "`np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n",
      "    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations. Did you mean: 'bool_'?\n",
      "]\n",
      "c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\_typing\\_array_like.py:84: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
      "  _ArrayLikeBool_co: TypeAlias = _DualArrayLike[dtype[np.bool], bool]\n",
      "[autoreload of numpy._typing._array_like failed: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\importlib\\__init__.py\", line 131, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 866, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 995, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\_typing\\_array_like.py\", line 84, in <module>\n",
      "    _ArrayLikeBool_co: TypeAlias = _DualArrayLike[dtype[np.bool], bool]\n",
      "                                                        ^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\__init__.py\", line 324, in __getattr__\n",
      "    linspace,\n",
      "        ^^^^^^\n",
      "AttributeError: module 'numpy' has no attribute 'bool'.\n",
      "`np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n",
      "    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations. Did you mean: 'bool_'?\n",
      "]\n",
      "[autoreload of numpy._typing failed: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\importlib\\__init__.py\", line 131, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 866, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 995, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\_typing\\__init__.py\", line 5, in <module>\n",
      "    from ._array_like import (\n",
      "ImportError: cannot import name '_ArrayLikeAnyString_co' from 'numpy._typing._array_like' (c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\_typing\\_array_like.py)\n",
      "]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Using cached numpy-2.4.2-cp312-cp312-win_amd64.whl.metadata (6.6 kB)\n",
      "Using cached numpy-2.4.2-cp312-cp312-win_amd64.whl (12.3 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.4.2\n",
      "    Uninstalling numpy-2.4.2:\n",
      "      Successfully uninstalled numpy-2.4.2\n",
      "Successfully installed numpy-2.4.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "contourpy 1.2.0 requires numpy<2.0,>=1.20, but you have numpy 2.4.2 which is incompatible.\n",
      "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.4.2 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.4.2 which is incompatible.\n",
      "scipy 1.13.1 requires numpy<2.3,>=1.22.4, but you have numpy 2.4.2 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --force-reinstall numpy\n",
    "\n",
    "import numpy as np\n",
    "import string\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#################################\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of numpy.core failed: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 455, in superreload\n",
      "    if not append_obj(module, old_objects, name, obj):\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 423, in append_obj\n",
      "    in_module = hasattr(obj, \"__module__\") and obj.__module__ == module.__name__\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\core\\defchararray.py\", line 2, in __getattr__\n",
      "    from numpy._core import defchararray\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\_core\\defchararray.py\", line 23, in <module>\n",
      "    from numpy._core.strings import (\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\_core\\strings.py\", line 93, in <module>\n",
      "    MAX = np.iinfo(np.int64).max\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\core\\getlimits.py\", line 687, in __init__\n",
      "NameError: name 'numeric' is not defined\n",
      "]\n",
      "[autoreload of numpy failed: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 455, in superreload\n",
      "    if not append_obj(module, old_objects, name, obj):\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 423, in append_obj\n",
      "    in_module = hasattr(obj, \"__module__\") and obj.__module__ == module.__name__\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\core\\defchararray.py\", line 2, in __getattr__\n",
      "    from numpy._core import defchararray\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\_core\\defchararray.py\", line 23, in <module>\n",
      "    from numpy._core.strings import (\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\_core\\strings.py\", line 93, in <module>\n",
      "    MAX = np.iinfo(np.int64).max\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\core\\getlimits.py\", line 687, in __init__\n",
      "NameError: name 'numeric' is not defined\n",
      "]\n",
      "[autoreload of numpy._core._add_newdocs failed: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\importlib\\__init__.py\", line 131, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 866, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 995, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\_core\\_add_newdocs.py\", line 7084, in <module>\n",
      "    add_newdoc('numpy.dtypes', _dtype_name,\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\_core\\function_base.py\", line 537, in add_newdoc\n",
      "    new = getattr(__import__(place, globals(), {}, [obj]), obj)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: module 'numpy.dtypes' has no attribute 'BoolDType'\n",
      "]\n",
      "c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\ma\\core.py:86: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
      "  MaskType = np.bool\n",
      "[autoreload of numpy.ma.core failed: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\importlib\\__init__.py\", line 131, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 866, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 995, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\ma\\core.py\", line 86, in <module>\n",
      "    MaskType = np.bool\n",
      "               ^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\__init__.py\", line 324, in __getattr__\n",
      "    linspace,\n",
      "        ^^^^^^\n",
      "AttributeError: module 'numpy' has no attribute 'bool'.\n",
      "`np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n",
      "    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations. Did you mean: 'bool_'?\n",
      "]\n",
      "c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\_typing\\_scalars.py:11: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
      "  _BoolLike_co: TypeAlias = bool | np.bool\n",
      "[autoreload of numpy._typing._scalars failed: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\importlib\\__init__.py\", line 131, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 866, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 995, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\_typing\\_scalars.py\", line 11, in <module>\n",
      "    _BoolLike_co: TypeAlias = bool | np.bool\n",
      "                                     ^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\__init__.py\", line 324, in __getattr__\n",
      "    linspace,\n",
      "        ^^^^^^\n",
      "AttributeError: module 'numpy' has no attribute 'bool'.\n",
      "`np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n",
      "    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations. Did you mean: 'bool_'?\n",
      "]\n",
      "c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\_typing\\_dtype_like.py:83: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
      "  _DTypeLikeBool: TypeAlias = type[bool] | _DTypeLike[np.bool] | _BoolCodes\n",
      "[autoreload of numpy._typing._dtype_like failed: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\importlib\\__init__.py\", line 131, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 866, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 995, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\_typing\\_dtype_like.py\", line 83, in <module>\n",
      "    _DTypeLikeBool: TypeAlias = type[bool] | _DTypeLike[np.bool] | _BoolCodes\n",
      "                                                        ^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\__init__.py\", line 324, in __getattr__\n",
      "    linspace,\n",
      "        ^^^^^^\n",
      "AttributeError: module 'numpy' has no attribute 'bool'.\n",
      "`np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n",
      "    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations. Did you mean: 'bool_'?\n",
      "]\n",
      "c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\_typing\\_array_like.py:84: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
      "  _ArrayLikeBool_co: TypeAlias = _DualArrayLike[dtype[np.bool], bool]\n",
      "[autoreload of numpy._typing._array_like failed: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\importlib\\__init__.py\", line 131, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 866, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 995, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\_typing\\_array_like.py\", line 84, in <module>\n",
      "    _ArrayLikeBool_co: TypeAlias = _DualArrayLike[dtype[np.bool], bool]\n",
      "                                                        ^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\__init__.py\", line 324, in __getattr__\n",
      "    linspace,\n",
      "        ^^^^^^\n",
      "AttributeError: module 'numpy' has no attribute 'bool'.\n",
      "`np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n",
      "    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations. Did you mean: 'bool_'?\n",
      "]\n",
      "[autoreload of numpy._typing failed: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\importlib\\__init__.py\", line 131, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 866, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 995, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\_typing\\__init__.py\", line 5, in <module>\n",
      "    from ._array_like import (\n",
      "ImportError: cannot import name '_ArrayLikeAnyString_co' from 'numpy._typing._array_like' (c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\_typing\\_array_like.py)\n",
      "]\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: '\\x07sl_data\\\\sign_mnist_train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Define the path to your data folder (update if needed)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m DATA_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\a\u001b[39;00m\u001b[38;5;124msl_data\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# or the correct path to your data\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m train_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(DATA_PATH, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msign_mnist_train.csv\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      5\u001b[0m valid_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(DATA_PATH, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msign_mnist_valid.csv\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32mc:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 22] Invalid argument: '\\x07sl_data\\\\sign_mnist_train.csv'"
     ]
    }
   ],
   "source": [
    "# Define the path to your data folder (update if needed)\n",
    "DATA_PATH = '\\asl_data'  # or the correct path to your data\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(DATA_PATH, 'sign_mnist_train.csv'))\n",
    "valid_df = pd.read_csv(os.path.join(DATA_PATH, 'sign_mnist_valid.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importar Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(train_df['label'])\n",
    "y_val = np.array(valid_df['label'])\n",
    "del train_df['label']\n",
    "del valid_df['label']\n",
    "x_train = train_df.values.astype(np.float32)\n",
    "x_val = valid_df.values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def split_val_test(x, y, pct=0.5, shuffle=True):\n",
    "    '''\n",
    "    Split the dataset into validation and test sets.\n",
    "    \n",
    "    Parameters:\n",
    "    x: numpy array of features\n",
    "    y: numpy array of labels\n",
    "    pct: percentage of data to use for validation (default 0.5)\n",
    "    shuffle: whether to shuffle the data before splitting (default True)\n",
    "    \n",
    "    Returns:\n",
    "    x_val, y_val, x_test, y_test: split datasets\n",
    "    '''\n",
    "    \n",
    "    # Get the number of samples\n",
    "    num_samples = len(x)\n",
    "    \n",
    "    # Create indices array\n",
    "    indices = np.arange(num_samples)\n",
    "    \n",
    "    # Shuffle if requested\n",
    "    if shuffle:\n",
    "        np.random.seed(42)  # For reproducibility\n",
    "        np.random.shuffle(indices)\n",
    "    \n",
    "    # Calculate split point\n",
    "    split_point = int(num_samples * pct)\n",
    "    \n",
    "    # Split indices\n",
    "    val_indices = indices[:split_point]\n",
    "    test_indices = indices[split_point:]\n",
    "    \n",
    "    # Split the data\n",
    "    x_val_split = x[val_indices]\n",
    "    y_val_split = y[val_indices]\n",
    "    \n",
    "    x_test = x[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    return x_val_split, y_val_split, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the validation set into new validation and test sets\n",
    "x_val_new, y_val_new, x_test, y_test = split_val_test(x_val, y_val, pct=0.5, shuffle=True)\n",
    "\n",
    "# Print the sizes of the new datasets\n",
    "print(f\"Original validation size: {len(x_val)}\")\n",
    "print(f\"New validation size: {len(x_val_new)}\")\n",
    "print(f\"Test size: {len(x_test)}\")\n",
    "print(f\"Label distribution in new validation: {np.bincount(y_val_new)}\")\n",
    "print(f\"Label distribution in test: {np.bincount(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### The following\n",
    "\n",
    "alphabet=list(string.ascii_lowercase)\n",
    "alphabet.remove('j')\n",
    "alphabet.remove('z')\n",
    "print(len(alphabet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(x, method='minmax', range=(0, 1)):\n",
    "    '''\n",
    "    Normalize data using different methods.\n",
    "    \n",
    "    Parameters:\n",
    "    x: numpy array of data\n",
    "    method: normalization method ('minmax', 'meanstd', or 'scale255')\n",
    "            - 'minmax': scale to specified range (default 0-1)\n",
    "            - 'meanstd': zero mean, unit variance\n",
    "            - 'scale255': simple divide by 255 (for 0-255 pixel values)\n",
    "    range: tuple for min and max range when using 'minmax' method\n",
    "    \n",
    "    Returns:\n",
    "    Normalized array\n",
    "    '''\n",
    "    \n",
    "    if method == 'scale255':\n",
    "        # Simple normalization for pixel values (0-255)\n",
    "        return x / 255.0\n",
    "    \n",
    "    elif method == 'minmax':\n",
    "        # Min-max normalization to specified range\n",
    "        x_min = x.min()\n",
    "        x_max = x.max()\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        if x_max - x_min == 0:\n",
    "            return np.zeros_like(x)\n",
    "        \n",
    "        x_norm = (x - x_min) / (x_max - x_min)\n",
    "        \n",
    "        # Scale to desired range\n",
    "        range_min, range_max = range\n",
    "        return x_norm * (range_max - range_min) + range_min\n",
    "    \n",
    "    elif method == 'meanstd':\n",
    "        # Standardization (zero mean, unit variance)\n",
    "        mean = x.mean()\n",
    "        std = x.std()\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        if std == 0:\n",
    "            return np.zeros_like(x)\n",
    "        \n",
    "        return (x - mean) / std\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown normalization method: {method}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Simple normalization (0-255 pixel values)\n",
    "#x_train_norm = normalise(x_train, method='scale255')\n",
    "\n",
    "# Option 2: Min-max normalization (values between 0 and 1)\n",
    "#x_train_norm = normalise(x_train, method='minmax', range=(0, 1))\n",
    "\n",
    "# Option 3: Normalization between -1 and 1 (useful for some neural networks)\n",
    "#x_train_norm = normalise(x_train, method='minmax', range=(-1, 1))\n",
    "\n",
    "# Option 4: Standardization (mean=0, std=1)\n",
    "#x_train_norm = normalise(x_train, method='meanstd')\n",
    "\n",
    "x_train = normalise(x_train, method='scale255')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graficar muestras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_number(img, label=None):\n",
    "    '''\n",
    "    Plot a 28x28 image (handwritten sign).\n",
    "    \n",
    "    Parameters:\n",
    "    img: 28x28 numpy array representing the image\n",
    "    label: optional label to display in title\n",
    "    '''\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    if label is not None:\n",
    "        plt.title(f'Label: {label}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Display some sample images from the training set\n",
    "fig, axes = plt.subplots(3, 4, figsize=(12, 9))\n",
    "fig.suptitle('Sample ASL Sign Images', fontsize=16)\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    idx = np.random.randint(len(x_train))\n",
    "    img = x_train[idx].reshape(28, 28)\n",
    "    label = alphabet[y_train[idx]]\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    ax.set_title(f'{label}')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ecuaciones para nuestro modelo\n",
    "\n",
    "\n",
    "$$z^1 = W^1 X + b^1$$\n",
    "\n",
    "$$a^1 = ReLU(z^1) $$\n",
    "\n",
    "$$z^2 = W^2 a^1 + b^2$$\n",
    "\n",
    "$$\\hat{y} = \\frac{e^{z^{2_k}}}{\\sum_j{e^{z_j}}}$$\n",
    "\n",
    "\n",
    "$$ \\mathcal{L}(\\hat{y}^{i}, y^{i}) =  - y^{i}  \\ln(\\hat{y}^{i}) = -\\ln(\\hat{y}^i)$$\n",
    "\n",
    "\n",
    "$$ \\mathcal{J}(w, b) =  \\frac{1}{num\\_samples} \\sum_{i=1}^{num\\_samples}-\\ln(\\hat{y}^{i})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones adicionales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mini batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batches(x, y, batch_size=32, shuffle=True):\n",
    "    '''\n",
    "    Create mini batches from the dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    x: numpy array of features\n",
    "    y: numpy array of labels\n",
    "    batch_size: size of each mini batch (default 32)\n",
    "    shuffle: whether to shuffle the data before creating batches (default True)\n",
    "    \n",
    "    Returns:\n",
    "    List of tuples (x_batch, y_batch)\n",
    "    '''\n",
    "    \n",
    "    num_samples = len(x)\n",
    "    indices = np.arange(num_samples)\n",
    "    \n",
    "    # Shuffle if requested\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "    \n",
    "    # Create mini batches\n",
    "    mini_batches = []\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        batch_indices = indices[i:i + batch_size]\n",
    "        x_batch = x[batch_indices]\n",
    "        y_batch = y[batch_indices]\n",
    "        mini_batches.append((x_batch, y_batch))\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nuestra clase Linear, ReLU y Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Clase Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    '''\n",
    "    Linear (fully connected) layer implementation.\n",
    "    \n",
    "    Parameters:\n",
    "    input_size: number of input features\n",
    "    output_size: number of output features (neurons)\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, input_size, output_size):\n",
    "        '''\n",
    "        Initialize the linear layer with weights and biases.\n",
    "        \n",
    "        Weights are initialized with small random values (Xavier/He initialization).\n",
    "        Biases are initialized to zeros.\n",
    "        '''\n",
    "        # Xavier initialization for weights\n",
    "        limit = np.sqrt(6 / (input_size + output_size))\n",
    "        self.W = np.random.uniform(-limit, limit, size=(input_size, output_size)).astype(np.float32)\n",
    "        self.b = np.zeros((1, output_size)).astype(np.float32)\n",
    "        \n",
    "        # Store gradients for backpropagation\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "        # Store input for backpropagation\n",
    "        self.X = None\n",
    "    \n",
    "    def forward(self, X):\n",
    "        '''\n",
    "        Forward pass: compute Z = X @ W + b\n",
    "        \n",
    "        Parameters:\n",
    "        X: input data of shape (batch_size, input_size)\n",
    "        \n",
    "        Returns:\n",
    "        Z: output of shape (batch_size, output_size)\n",
    "        '''\n",
    "        self.X = X\n",
    "        Z = X @ self.W + self.b\n",
    "        return Z\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        '''\n",
    "        Backward pass: compute gradients with respect to W, b, and X\n",
    "        \n",
    "        Parameters:\n",
    "        dZ: gradient of loss with respect to Z, shape (batch_size, output_size)\n",
    "        \n",
    "        Returns:\n",
    "        dX: gradient of loss with respect to X, shape (batch_size, input_size)\n",
    "        '''\n",
    "        batch_size = self.X.shape[0]\n",
    "        \n",
    "        # Gradients with respect to weights and biases\n",
    "        self.dW = (self.X.T @ dZ) / batch_size\n",
    "        self.db = np.sum(dZ, axis=0, keepdims=True) / batch_size\n",
    "        \n",
    "        # Gradient with respect to input\n",
    "        dX = dZ @ self.W.T\n",
    "        \n",
    "        return dX\n",
    "    \n",
    "    def update_parameters(self, learning_rate):\n",
    "        '''\n",
    "        Update weights and biases using gradient descent.\n",
    "        \n",
    "        Parameters:\n",
    "        learning_rate: learning rate for the update\n",
    "        '''\n",
    "        self.W -= learning_rate * self.dW\n",
    "        self.b -= learning_rate * self.db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clase ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    '''\n",
    "    ReLU (Rectified Linear Unit) activation function implementation.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Initialize the ReLU layer.\n",
    "        Store input for backpropagation.\n",
    "        '''\n",
    "        self.Z = None\n",
    "    \n",
    "    def forward(self, Z):\n",
    "        '''\n",
    "        Forward pass: compute ReLU activation\n",
    "        ReLU(Z) = max(0, Z)\n",
    "        \n",
    "        Parameters:\n",
    "        Z: input data of shape (batch_size, num_features)\n",
    "        \n",
    "        Returns:\n",
    "        A: activated output of shape (batch_size, num_features)\n",
    "        '''\n",
    "        self.Z = Z\n",
    "        A = np.maximum(0, Z)\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        '''\n",
    "        Backward pass: compute gradient with respect to Z\n",
    "        \n",
    "        Parameters:\n",
    "        dA: gradient of loss with respect to A, shape (batch_size, num_features)\n",
    "        \n",
    "        Returns:\n",
    "        dZ: gradient of loss with respect to Z, shape (batch_size, num_features)\n",
    "        '''\n",
    "        # Gradient is 1 where Z > 0, and 0 where Z <= 0\n",
    "        dZ = dA * (self.Z > 0).astype(np.float32)\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clase Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential:\n",
    "    '''\n",
    "    Sequential model that stacks multiple layers.\n",
    "    Supports forward pass, backward pass, and parameter updates.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, layers):\n",
    "        '''\n",
    "        Initialize the Sequential model with a list of layers.\n",
    "        \n",
    "        Parameters:\n",
    "        layers: list of layer objects (Linear, ReLU, etc.)\n",
    "        '''\n",
    "        self.layers = layers\n",
    "    \n",
    "    def forward(self, X):\n",
    "        '''\n",
    "        Forward pass through all layers.\n",
    "        \n",
    "        Parameters:\n",
    "        X: input data of shape (batch_size, input_size)\n",
    "        \n",
    "        Returns:\n",
    "        A: output after passing through all layers\n",
    "        '''\n",
    "        A = X\n",
    "        for layer in self.layers:\n",
    "            A = layer.forward(A)\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        '''\n",
    "        Backward pass through all layers in reverse order.\n",
    "        \n",
    "        Parameters:\n",
    "        dZ: gradient of loss with respect to the output\n",
    "        \n",
    "        Returns:\n",
    "        dX: gradient of loss with respect to the input\n",
    "        '''\n",
    "        dA = dZ\n",
    "        for layer in reversed(self.layers):\n",
    "            dA = layer.backward(dA)\n",
    "        return dA\n",
    "    \n",
    "    def update_parameters(self, learning_rate):\n",
    "        '''\n",
    "        Update parameters for all layers that have learnable parameters.\n",
    "        \n",
    "        Parameters:\n",
    "        learning_rate: learning rate for the update\n",
    "        '''\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'update_parameters'):\n",
    "                layer.update_parameters(learning_rate)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Make predictions on input data.\n",
    "        Returns the class with the highest probability.\n",
    "        \n",
    "        Parameters:\n",
    "        X: input data of shape (num_samples, input_size)\n",
    "        \n",
    "        Returns:\n",
    "        predictions: array of predicted class indices\n",
    "        '''\n",
    "        output = self.forward(X)\n",
    "        return np.argmax(output, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    '''\n",
    "    Compute softmax activation for multi-class classification.\n",
    "    \n",
    "    Parameters:\n",
    "    Z: numpy array of shape (batch_size, num_classes)\n",
    "    \n",
    "    Returns:\n",
    "    A: softmax probabilities of shape (batch_size, num_classes)\n",
    "    '''\n",
    "    # Subtract max for numerical stability\n",
    "    Z_shifted = Z - np.max(Z, axis=1, keepdims=True)\n",
    "    exp_Z = np.exp(Z_shifted)\n",
    "    A = exp_Z / np.sum(exp_Z, axis=1, keepdims=True)\n",
    "    return A\n",
    "\n",
    "\n",
    "def cross_entropy_loss(y_pred, y_true):\n",
    "    '''\n",
    "    Compute cross-entropy loss for multi-class classification.\n",
    "    \n",
    "    Parameters:\n",
    "    y_pred: softmax probabilities of shape (batch_size, num_classes)\n",
    "    y_true: true labels (class indices) of shape (batch_size,)\n",
    "    \n",
    "    Returns:\n",
    "    loss: average cross-entropy loss across the batch\n",
    "    '''\n",
    "    batch_size = len(y_true)\n",
    "    # Get probabilities of true classes\n",
    "    correct_logprobs = -np.log(y_pred[np.arange(batch_size), y_true])\n",
    "    loss = np.mean(correct_logprobs)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def compute_loss_and_gradients(y_pred, y_true):\n",
    "    '''\n",
    "    Compute loss and gradients with respect to predictions.\n",
    "    \n",
    "    Parameters:\n",
    "    y_pred: softmax probabilities of shape (batch_size, num_classes)\n",
    "    y_true: true labels (class indices) of shape (batch_size,)\n",
    "    \n",
    "    Returns:\n",
    "    loss: cross-entropy loss\n",
    "    dZ: gradient of loss with respect to pre-softmax output\n",
    "    '''\n",
    "    batch_size = len(y_true)\n",
    "    loss = cross_entropy_loss(y_pred, y_true)\n",
    "    \n",
    "    # Gradient of cross-entropy with softmax\n",
    "    dZ = y_pred.copy()\n",
    "    dZ[np.arange(batch_size), y_true] -= 1\n",
    "    \n",
    "    return loss, dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with dynamic architecture\n",
    "def train_model(model, x_train, y_train, x_val, y_val, epochs=50, batch_size=32, learning_rate=0.01):\n",
    "    '''\n",
    "    Train the neural network model.\n",
    "    \n",
    "    Parameters:\n",
    "    model: Sequential model to train\n",
    "    x_train: training features\n",
    "    y_train: training labels\n",
    "    x_val: validation features\n",
    "    y_val: validation labels\n",
    "    epochs: number of training epochs\n",
    "    batch_size: size of mini batches\n",
    "    learning_rate: learning rate for gradient descent\n",
    "    \n",
    "    Returns:\n",
    "    history: dictionary with training metrics\n",
    "    '''\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    num_classes = len(np.unique(y_train))\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Create mini batches\n",
    "        mini_batches = create_mini_batches(x_train, y_train, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        epoch_loss = 0\n",
    "        epoch_correct = 0\n",
    "        epoch_total = 0\n",
    "        \n",
    "        # Training phase\n",
    "        for x_batch, y_batch in mini_batches:\n",
    "            # Forward pass\n",
    "            logits = model.forward(x_batch)\n",
    "            predictions = softmax(logits)\n",
    "            \n",
    "            # Compute loss and gradients\n",
    "            batch_loss, dZ = compute_loss_and_gradients(predictions, y_batch)\n",
    "            epoch_loss += batch_loss * len(y_batch)\n",
    "            \n",
    "            # Count correct predictions\n",
    "            batch_pred = np.argmax(predictions, axis=1)\n",
    "            epoch_correct += np.sum(batch_pred == y_batch)\n",
    "            epoch_total += len(y_batch)\n",
    "            \n",
    "            # Backward pass\n",
    "            model.backward(dZ)\n",
    "            \n",
    "            # Update parameters\n",
    "            model.update_parameters(learning_rate)\n",
    "        \n",
    "        # Calculate epoch metrics\n",
    "        train_loss = epoch_loss / epoch_total\n",
    "        train_acc = epoch_correct / epoch_total\n",
    "        \n",
    "        # Validation phase\n",
    "        val_logits = model.forward(x_val)\n",
    "        val_predictions = softmax(val_logits)\n",
    "        val_loss = cross_entropy_loss(val_predictions, y_val)\n",
    "        val_pred = np.argmax(val_predictions, axis=1)\n",
    "        val_acc = np.sum(val_pred == y_val) / len(y_val)\n",
    "        \n",
    "        # Store history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs} - Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create your model and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define model architecture dynamically\n",
    "input_size = x_train.shape[1]  # 784 for 28x28 images\n",
    "num_classes = len(alphabet)  # 24 classes\n",
    "\n",
    "# Create model with dynamic layers\n",
    "model = Sequential([\n",
    "    Linear(input_size, 128),\n",
    "    ReLU(),\n",
    "    Linear(128, 64),\n",
    "    ReLU(),\n",
    "    Linear(64, num_classes)\n",
    "])\n",
    "\n",
    "# Normalize validation and test data with same method\n",
    "x_val_new = normalise(x_val_new, method='scale255')\n",
    "x_test = normalise(x_test, method='scale255')\n",
    "\n",
    "# Train the model\n",
    "history = train_model(model, x_train, y_train, x_val_new, y_val_new, \n",
    "                      epochs=50, batch_size=32, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your model on Random data from your test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.randint(len(y_test))\n",
    "plot_number(x_test[idx].reshape(28,28))\n",
    "pred = model.predict(x_test[idx].reshape(-1, 1))\n",
    "print(f'el valor predicho es: {alphabet[pred]} el valor real es:{alphabet[y_test[idx]]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
