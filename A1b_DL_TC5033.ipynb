{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TC 5033\n",
    "## Deep Learning\n",
    "## Fully Connected Deep Neural Networks\n",
    "\n",
    "#### Activity 1b: Implementing a Fully Connected Network for Kaggle ASL Dataset\n",
    "\n",
    "- Objective\n",
    "\n",
    "The aim of this part of the activity is to apply your understanding of Fully Connected Networks by implementing a multilayer network for the [Kaggle ASL (American Sign Language) dataset](https://www.kaggle.com/datasets/grassknoted/asl-alphabet). While you have been provided with a complete solution for a Fully Connected Network using Numpy for the MNIST dataset, you are encouraged to try to come up with the solution.\n",
    "\n",
    "- Instructions\n",
    "\n",
    "    This activity requires submission in teams of 3 or 4 members. Submissions from smaller or larger teams will not be accepted unless prior approval has been granted (only due to exceptional circumstances). While teamwork is encouraged, each member is expected to contribute individually to the assignment. The final submission should feature the best arguments and solutions from each team member. Only one person per team needs to submit the completed work, but it is imperative that the names of all team members are listed in a Markdown cell at the very beginning of the notebook (either the first or second cell). Failure to include all team member names will result in the grade being awarded solely to the individual who submitted the assignment, with zero points given to other team members (no exceptions will be made to this rule).\n",
    "\n",
    "    Load and Preprocess Data: You are provided a starter code to load the data. Be sure to understand the code.\n",
    "\n",
    "    Review MNIST Notebook (Optional): Before diving into this activity, you have the option to revisit the MNIST example to refresh your understanding of how to build a Fully Connected Network using Numpy.\n",
    "\n",
    "    Start Fresh: Although you can refer to the MNIST solution at any point, try to implement the network for the ASL dataset on your own. This will reinforce your learning and understanding of the architecture and mathematics involved.\n",
    "\n",
    "    Implement Forward and Backward Pass: Write the code to perform the forward and backward passes, keeping in mind the specific challenges and characteristics of the ASL dataset.\n",
    "    \n",
    "     Design the Network: Create the architecture of the Fully Connected Network tailored for the ASL dataset. Choose the number of hidden layers, neurons, and hyperparameters judiciously.\n",
    "\n",
    "    Train the Model: Execute the training loop, ensuring to track performance metrics such as loss and accuracy.\n",
    "\n",
    "    Analyze and Document: Use Markdown cells to document in detail the choices you made in terms of architecture and hyperparameters, you may use figures, equations, etc to aid in your explanations. Include any metrics that help justify these choices and discuss the model's performance.  \n",
    "\n",
    "- Evaluation Criteria\n",
    "\n",
    "    - Code Readability and Comments\n",
    "    - Appropriateness of chosen architecture and hyperparameters for the ASL dataset\n",
    "    - Performance of the model on the ASL dataset (at least 70% acc)\n",
    "    - Quality of Markdown documentation\n",
    "\n",
    "- Submission\n",
    "\n",
    "Submit this Jupyter Notebook in canvas with your complete solution, ensuring your code is well-commented and includes Markdown cells that explain your design choices, results, and any challenges you encountered.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\hjma9\\anaconda3\\envs\\env_pyspark\\lib\\site-packages (4.13.0.90)\n",
      "Requirement already satisfied: numpy>=2 in c:\\users\\hjma9\\anaconda3\\envs\\env_pyspark\\lib\\site-packages (from opencv-python) (2.4.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Using cached numpy-2.4.2-cp312-cp312-win_amd64.whl.metadata (6.6 kB)\n",
      "Using cached numpy-2.4.2-cp312-cp312-win_amd64.whl (12.3 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.4.2\n",
      "    Uninstalling numpy-2.4.2:\n",
      "      Successfully uninstalled numpy-2.4.2\n",
      "Successfully installed numpy-2.4.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "contourpy 1.2.0 requires numpy<2.0,>=1.20, but you have numpy 2.4.2 which is incompatible.\n",
      "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.4.2 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.4.2 which is incompatible.\n",
      "scipy 1.13.1 requires numpy<2.3,>=1.22.4, but you have numpy 2.4.2 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --force-reinstall numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of numpy.core failed: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 455, in superreload\n",
      "    if not append_obj(module, old_objects, name, obj):\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 423, in append_obj\n",
      "    in_module = hasattr(obj, \"__module__\") and obj.__module__ == module.__name__\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\core\\defchararray.py\", line 2, in __getattr__\n",
      "    from numpy._core import defchararray\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\_core\\defchararray.py\", line 23, in <module>\n",
      "    from numpy._core.strings import (\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\_core\\strings.py\", line 93, in <module>\n",
      "    MAX = np.iinfo(np.int64).max\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\core\\getlimits.py\", line 687, in __init__\n",
      "NameError: name 'numeric' is not defined\n",
      "]\n",
      "[autoreload of numpy failed: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 455, in superreload\n",
      "    if not append_obj(module, old_objects, name, obj):\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 423, in append_obj\n",
      "    in_module = hasattr(obj, \"__module__\") and obj.__module__ == module.__name__\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\core\\defchararray.py\", line 2, in __getattr__\n",
      "    from numpy._core import defchararray\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\_core\\defchararray.py\", line 23, in <module>\n",
      "    from numpy._core.strings import (\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\_core\\strings.py\", line 93, in <module>\n",
      "    MAX = np.iinfo(np.int64).max\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\core\\getlimits.py\", line 687, in __init__\n",
      "NameError: name 'numeric' is not defined\n",
      "]\n",
      "[autoreload of numpy._core._add_newdocs failed: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\importlib\\__init__.py\", line 131, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 866, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 995, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\_core\\_add_newdocs.py\", line 7084, in <module>\n",
      "    add_newdoc('numpy.dtypes', _dtype_name,\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\_core\\function_base.py\", line 537, in add_newdoc\n",
      "    new = getattr(__import__(place, globals(), {}, [obj]), obj)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: module 'numpy.dtypes' has no attribute 'BoolDType'\n",
      "]\n",
      "c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\ma\\core.py:86: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
      "  MaskType = np.bool\n",
      "[autoreload of numpy.ma.core failed: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\importlib\\__init__.py\", line 131, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 866, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 995, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\ma\\core.py\", line 86, in <module>\n",
      "    MaskType = np.bool\n",
      "               ^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\__init__.py\", line 324, in __getattr__\n",
      "    linspace,\n",
      "        ^^^^^^\n",
      "AttributeError: module 'numpy' has no attribute 'bool'.\n",
      "`np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n",
      "    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations. Did you mean: 'bool_'?\n",
      "]\n",
      "c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\_typing\\_scalars.py:11: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
      "  _BoolLike_co: TypeAlias = bool | np.bool\n",
      "[autoreload of numpy._typing._scalars failed: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\importlib\\__init__.py\", line 131, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 866, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 995, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\_typing\\_scalars.py\", line 11, in <module>\n",
      "    _BoolLike_co: TypeAlias = bool | np.bool\n",
      "                                     ^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\__init__.py\", line 324, in __getattr__\n",
      "    linspace,\n",
      "        ^^^^^^\n",
      "AttributeError: module 'numpy' has no attribute 'bool'.\n",
      "`np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n",
      "    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations. Did you mean: 'bool_'?\n",
      "]\n",
      "c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\_typing\\_dtype_like.py:83: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
      "  _DTypeLikeBool: TypeAlias = type[bool] | _DTypeLike[np.bool] | _BoolCodes\n",
      "[autoreload of numpy._typing._dtype_like failed: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\importlib\\__init__.py\", line 131, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 866, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 995, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\_typing\\_dtype_like.py\", line 83, in <module>\n",
      "    _DTypeLikeBool: TypeAlias = type[bool] | _DTypeLike[np.bool] | _BoolCodes\n",
      "                                                        ^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\__init__.py\", line 324, in __getattr__\n",
      "    linspace,\n",
      "        ^^^^^^\n",
      "AttributeError: module 'numpy' has no attribute 'bool'.\n",
      "`np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n",
      "    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations. Did you mean: 'bool_'?\n",
      "]\n",
      "c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\_typing\\_array_like.py:84: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
      "  _ArrayLikeBool_co: TypeAlias = _DualArrayLike[dtype[np.bool], bool]\n",
      "[autoreload of numpy._typing._array_like failed: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\importlib\\__init__.py\", line 131, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 866, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 995, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\_typing\\_array_like.py\", line 84, in <module>\n",
      "    _ArrayLikeBool_co: TypeAlias = _DualArrayLike[dtype[np.bool], bool]\n",
      "                                                        ^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\__init__.py\", line 324, in __getattr__\n",
      "    linspace,\n",
      "        ^^^^^^\n",
      "AttributeError: module 'numpy' has no attribute 'bool'.\n",
      "`np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n",
      "    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations. Did you mean: 'bool_'?\n",
      "]\n",
      "[autoreload of numpy._typing failed: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\importlib\\__init__.py\", line 131, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 866, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 995, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\_typing\\__init__.py\", line 5, in <module>\n",
      "    from ._array_like import (\n",
      "ImportError: cannot import name '_ArrayLikeAnyString_co' from 'numpy._typing._array_like' (c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\_typing\\_array_like.py)\n",
      "]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Using cached numpy-2.4.2-cp312-cp312-win_amd64.whl.metadata (6.6 kB)\n",
      "Using cached numpy-2.4.2-cp312-cp312-win_amd64.whl (12.3 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.4.2\n",
      "    Uninstalling numpy-2.4.2:\n",
      "      Successfully uninstalled numpy-2.4.2\n",
      "Successfully installed numpy-2.4.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "contourpy 1.2.0 requires numpy<2.0,>=1.20, but you have numpy 2.4.2 which is incompatible.\n",
      "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.4.2 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.4.2 which is incompatible.\n",
      "scipy 1.13.1 requires numpy<2.3,>=1.22.4, but you have numpy 2.4.2 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --force-reinstall numpy\n",
    "\n",
    "import numpy as np\n",
    "import string\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#################################\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of numpy.core failed: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 455, in superreload\n",
      "    if not append_obj(module, old_objects, name, obj):\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 423, in append_obj\n",
      "    in_module = hasattr(obj, \"__module__\") and obj.__module__ == module.__name__\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\core\\defchararray.py\", line 2, in __getattr__\n",
      "    from numpy._core import defchararray\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\_core\\defchararray.py\", line 23, in <module>\n",
      "    from numpy._core.strings import (\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\_core\\strings.py\", line 93, in <module>\n",
      "    MAX = np.iinfo(np.int64).max\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\core\\getlimits.py\", line 687, in __init__\n",
      "NameError: name 'numeric' is not defined\n",
      "]\n",
      "[autoreload of numpy failed: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 455, in superreload\n",
      "    if not append_obj(module, old_objects, name, obj):\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 423, in append_obj\n",
      "    in_module = hasattr(obj, \"__module__\") and obj.__module__ == module.__name__\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\core\\defchararray.py\", line 2, in __getattr__\n",
      "    from numpy._core import defchararray\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\_core\\defchararray.py\", line 23, in <module>\n",
      "    from numpy._core.strings import (\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\_core\\strings.py\", line 93, in <module>\n",
      "    MAX = np.iinfo(np.int64).max\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\core\\getlimits.py\", line 687, in __init__\n",
      "NameError: name 'numeric' is not defined\n",
      "]\n",
      "[autoreload of numpy._core._add_newdocs failed: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\importlib\\__init__.py\", line 131, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 866, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 995, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\_core\\_add_newdocs.py\", line 7084, in <module>\n",
      "    add_newdoc('numpy.dtypes', _dtype_name,\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\_core\\function_base.py\", line 537, in add_newdoc\n",
      "    new = getattr(__import__(place, globals(), {}, [obj]), obj)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: module 'numpy.dtypes' has no attribute 'BoolDType'\n",
      "]\n",
      "c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\ma\\core.py:86: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
      "  MaskType = np.bool\n",
      "[autoreload of numpy.ma.core failed: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\importlib\\__init__.py\", line 131, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 866, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 995, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\ma\\core.py\", line 86, in <module>\n",
      "    MaskType = np.bool\n",
      "               ^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\__init__.py\", line 324, in __getattr__\n",
      "    linspace,\n",
      "        ^^^^^^\n",
      "AttributeError: module 'numpy' has no attribute 'bool'.\n",
      "`np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n",
      "    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations. Did you mean: 'bool_'?\n",
      "]\n",
      "c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\_typing\\_scalars.py:11: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
      "  _BoolLike_co: TypeAlias = bool | np.bool\n",
      "[autoreload of numpy._typing._scalars failed: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\importlib\\__init__.py\", line 131, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 866, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 995, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\_typing\\_scalars.py\", line 11, in <module>\n",
      "    _BoolLike_co: TypeAlias = bool | np.bool\n",
      "                                     ^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\__init__.py\", line 324, in __getattr__\n",
      "    linspace,\n",
      "        ^^^^^^\n",
      "AttributeError: module 'numpy' has no attribute 'bool'.\n",
      "`np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n",
      "    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations. Did you mean: 'bool_'?\n",
      "]\n",
      "c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\_typing\\_dtype_like.py:83: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
      "  _DTypeLikeBool: TypeAlias = type[bool] | _DTypeLike[np.bool] | _BoolCodes\n",
      "[autoreload of numpy._typing._dtype_like failed: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\importlib\\__init__.py\", line 131, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 866, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 995, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\_typing\\_dtype_like.py\", line 83, in <module>\n",
      "    _DTypeLikeBool: TypeAlias = type[bool] | _DTypeLike[np.bool] | _BoolCodes\n",
      "                                                        ^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\__init__.py\", line 324, in __getattr__\n",
      "    linspace,\n",
      "        ^^^^^^\n",
      "AttributeError: module 'numpy' has no attribute 'bool'.\n",
      "`np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n",
      "    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations. Did you mean: 'bool_'?\n",
      "]\n",
      "c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\_typing\\_array_like.py:84: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
      "  _ArrayLikeBool_co: TypeAlias = _DualArrayLike[dtype[np.bool], bool]\n",
      "[autoreload of numpy._typing._array_like failed: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\importlib\\__init__.py\", line 131, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 866, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 995, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\_typing\\_array_like.py\", line 84, in <module>\n",
      "    _ArrayLikeBool_co: TypeAlias = _DualArrayLike[dtype[np.bool], bool]\n",
      "                                                        ^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\__init__.py\", line 324, in __getattr__\n",
      "    linspace,\n",
      "        ^^^^^^\n",
      "AttributeError: module 'numpy' has no attribute 'bool'.\n",
      "`np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n",
      "    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations. Did you mean: 'bool_'?\n",
      "]\n",
      "[autoreload of numpy._typing failed: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\importlib\\__init__.py\", line 131, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 866, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 995, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "  File \"c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\_typing\\__init__.py\", line 5, in <module>\n",
      "    from ._array_like import (\n",
      "ImportError: cannot import name '_ArrayLikeAnyString_co' from 'numpy._typing._array_like' (c:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\_typing\\_array_like.py)\n",
      "]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'asarray' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Define the path to your data folder (update if needed)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m DATA_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./asl_data\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# or the correct path to your data\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m train_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(DATA_PATH, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msign_mnist_train.csv\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      5\u001b[0m valid_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(DATA_PATH, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msign_mnist_valid.csv\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\u001b[38;5;241m.\u001b[39mread(nrows)\n",
      "File \u001b[1;32mc:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1968\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1965\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1966\u001b[0m         new_col_dict \u001b[38;5;241m=\u001b[39m col_dict\n\u001b[1;32m-> 1968\u001b[0m     df \u001b[38;5;241m=\u001b[39m DataFrame(\n\u001b[0;32m   1969\u001b[0m         new_col_dict,\n\u001b[0;32m   1970\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   1971\u001b[0m         index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[0;32m   1972\u001b[0m         copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m using_copy_on_write(),\n\u001b[0;32m   1973\u001b[0m     )\n\u001b[0;32m   1975\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_currow \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_rows\n\u001b[0;32m   1976\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[1;32mc:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\pandas\\core\\frame.py:778\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    772\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    773\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    774\u001b[0m     )\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    777\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 778\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy, typ\u001b[38;5;241m=\u001b[39mmanager)\n\u001b[0;32m    779\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[1;32mc:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:443\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    441\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mseries\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Series\n\u001b[1;32m--> 443\u001b[0m     arrays \u001b[38;5;241m=\u001b[39m Series(data, index\u001b[38;5;241m=\u001b[39mcolumns, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mobject\u001b[39m)\n\u001b[0;32m    444\u001b[0m     missing \u001b[38;5;241m=\u001b[39m arrays\u001b[38;5;241m.\u001b[39misna()\n\u001b[0;32m    445\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    446\u001b[0m         \u001b[38;5;66;03m# GH10856\u001b[39;00m\n\u001b[0;32m    447\u001b[0m         \u001b[38;5;66;03m# raise ValueError if only scalars in dict\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\pandas\\core\\series.py:490\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[1;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[0;32m    487\u001b[0m name \u001b[38;5;241m=\u001b[39m ibase\u001b[38;5;241m.\u001b[39mmaybe_extract_name(name, data, \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m))\n\u001b[0;32m    489\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 490\u001b[0m     index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    493\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_dtype(dtype)\n",
      "File \u001b[1;32mc:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:7647\u001b[0m, in \u001b[0;36mensure_index\u001b[1;34m(index_like, copy)\u001b[0m\n\u001b[0;32m   7645\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m MultiIndex\u001b[38;5;241m.\u001b[39mfrom_arrays(index_like)\n\u001b[0;32m   7646\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 7647\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m Index(index_like, copy\u001b[38;5;241m=\u001b[39mcopy, tupleize_cols\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   7648\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   7649\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Index(index_like, copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[1;32mc:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:565\u001b[0m, in \u001b[0;36mIndex.__new__\u001b[1;34m(cls, data, dtype, copy, name, tupleize_cols)\u001b[0m\n\u001b[0;32m    562\u001b[0m         data \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39masarray_tuplesafe(data, dtype\u001b[38;5;241m=\u001b[39m_dtype_obj)\n\u001b[0;32m    564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 565\u001b[0m     arr \u001b[38;5;241m=\u001b[39m sanitize_array(data, \u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[0;32m    566\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    567\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex must be specified when data is not list-like\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(err):\n",
      "File \u001b[1;32mc:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\pandas\\core\\construction.py:654\u001b[0m, in \u001b[0;36msanitize_array\u001b[1;34m(data, index, dtype, copy, allow_2d)\u001b[0m\n\u001b[0;32m    651\u001b[0m     subarr \u001b[38;5;241m=\u001b[39m _try_cast(data, dtype, copy)\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 654\u001b[0m     subarr \u001b[38;5;241m=\u001b[39m maybe_convert_platform(data)\n\u001b[0;32m    655\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m subarr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[0;32m    656\u001b[0m         subarr \u001b[38;5;241m=\u001b[39m cast(np\u001b[38;5;241m.\u001b[39mndarray, subarr)\n",
      "File \u001b[1;32mc:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\pandas\\core\\dtypes\\cast.py:139\u001b[0m, in \u001b[0;36mmaybe_convert_platform\u001b[1;34m(values)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m _dtype_obj:\n\u001b[0;32m    138\u001b[0m     arr \u001b[38;5;241m=\u001b[39m cast(np\u001b[38;5;241m.\u001b[39mndarray, arr)\n\u001b[1;32m--> 139\u001b[0m     arr \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mmaybe_convert_objects(arr)\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr\n",
      "File \u001b[1;32mlib.pyx:2543\u001b[0m, in \u001b[0;36mpandas._libs.lib.maybe_convert_objects\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\hjma9\\anaconda3\\envs\\env_pyspark\\Lib\\site-packages\\numpy\\core\\numeric.py:327\u001b[0m, in \u001b[0;36mfull\u001b[1;34m(shape, fill_value, dtype, order, like)\u001b[0m\n\u001b[0;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[1;31mNameError\u001b[0m: name 'asarray' is not defined"
     ]
    }
   ],
   "source": [
    "# Define the path to your data folder (update if needed)\n",
    "DATA_PATH = '\\asl_data'  # or the correct path to your data\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(DATA_PATH, 'sign_mnist_train.csv'))\n",
    "valid_df = pd.read_csv(os.path.join(DATA_PATH, 'sign_mnist_valid.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importar Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(train_df['label'])\n",
    "y_val = np.array(valid_df['label'])\n",
    "del train_df['label']\n",
    "del valid_df['label']\n",
    "x_train = train_df.values.astype(np.float32)\n",
    "x_val = valid_df.values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def split_val_test(x, y, pct=0.5, shuffle=True):\n",
    "    '''\n",
    "    Split the dataset into validation and test sets.\n",
    "    \n",
    "    Parameters:\n",
    "    x: numpy array of features\n",
    "    y: numpy array of labels\n",
    "    pct: percentage of data to use for validation (default 0.5)\n",
    "    shuffle: whether to shuffle the data before splitting (default True)\n",
    "    \n",
    "    Returns:\n",
    "    x_val, y_val, x_test, y_test: split datasets\n",
    "    '''\n",
    "    \n",
    "    # Get the number of samples\n",
    "    num_samples = len(x)\n",
    "    \n",
    "    # Create indices array\n",
    "    indices = np.arange(num_samples)\n",
    "    \n",
    "    # Shuffle if requested\n",
    "    if shuffle:\n",
    "        np.random.seed(42)  # For reproducibility\n",
    "        np.random.shuffle(indices)\n",
    "    \n",
    "    # Calculate split point\n",
    "    split_point = int(num_samples * pct)\n",
    "    \n",
    "    # Split indices\n",
    "    val_indices = indices[:split_point]\n",
    "    test_indices = indices[split_point:]\n",
    "    \n",
    "    # Split the data\n",
    "    x_val_split = x[val_indices]\n",
    "    y_val_split = y[val_indices]\n",
    "    \n",
    "    x_test = x[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    return x_val_split, y_val_split, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the validation set into new validation and test sets\n",
    "x_val_new, y_val_new, x_test, y_test = split_val_test(x_val, y_val, pct=0.5, shuffle=True)\n",
    "\n",
    "# Print the sizes of the new datasets\n",
    "print(f\"Original validation size: {len(x_val)}\")\n",
    "print(f\"New validation size: {len(x_val_new)}\")\n",
    "print(f\"Test size: {len(x_test)}\")\n",
    "print(f\"Label distribution in new validation: {np.bincount(y_val_new)}\")\n",
    "print(f\"Label distribution in test: {np.bincount(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### The following\n",
    "\n",
    "alphabet=list(string.ascii_lowercase)\n",
    "alphabet.remove('j')\n",
    "alphabet.remove('z')\n",
    "print(len(alphabet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(x, method='minmax', range=(0, 1)):\n",
    "    '''\n",
    "    Normalize data using different methods.\n",
    "    \n",
    "    Parameters:\n",
    "    x: numpy array of data\n",
    "    method: normalization method ('minmax', 'meanstd', or 'scale255')\n",
    "            - 'minmax': scale to specified range (default 0-1)\n",
    "            - 'meanstd': zero mean, unit variance\n",
    "            - 'scale255': simple divide by 255 (for 0-255 pixel values)\n",
    "    range: tuple for min and max range when using 'minmax' method\n",
    "    \n",
    "    Returns:\n",
    "    Normalized array\n",
    "    '''\n",
    "    \n",
    "    if method == 'scale255':\n",
    "        # Simple normalization for pixel values (0-255)\n",
    "        return x / 255.0\n",
    "    \n",
    "    elif method == 'minmax':\n",
    "        # Min-max normalization to specified range\n",
    "        x_min = x.min()\n",
    "        x_max = x.max()\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        if x_max - x_min == 0:\n",
    "            return np.zeros_like(x)\n",
    "        \n",
    "        x_norm = (x - x_min) / (x_max - x_min)\n",
    "        \n",
    "        # Scale to desired range\n",
    "        range_min, range_max = range\n",
    "        return x_norm * (range_max - range_min) + range_min\n",
    "    \n",
    "    elif method == 'meanstd':\n",
    "        # Standardization (zero mean, unit variance)\n",
    "        mean = x.mean()\n",
    "        std = x.std()\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        if std == 0:\n",
    "            return np.zeros_like(x)\n",
    "        \n",
    "        return (x - mean) / std\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown normalization method: {method}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Simple normalization (0-255 pixel values)\n",
    "#x_train_norm = normalise(x_train, method='scale255')\n",
    "\n",
    "# Option 2: Min-max normalization (values between 0 and 1)\n",
    "#x_train_norm = normalise(x_train, method='minmax', range=(0, 1))\n",
    "\n",
    "# Option 3: Normalization between -1 and 1 (useful for some neural networks)\n",
    "#x_train_norm = normalise(x_train, method='minmax', range=(-1, 1))\n",
    "\n",
    "# Option 4: Standardization (mean=0, std=1)\n",
    "#x_train_norm = normalise(x_train, method='meanstd')\n",
    "\n",
    "x_train = normalise(x_train, method='scale255')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graficar muestras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_number(img, label=None):\n",
    "    '''\n",
    "    Plot a 28x28 image (handwritten sign).\n",
    "    \n",
    "    Parameters:\n",
    "    img: 28x28 numpy array representing the image\n",
    "    label: optional label to display in title\n",
    "    '''\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    if label is not None:\n",
    "        plt.title(f'Label: {label}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Display some sample images from the training set\n",
    "fig, axes = plt.subplots(3, 4, figsize=(12, 9))\n",
    "fig.suptitle('Sample ASL Sign Images', fontsize=16)\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    idx = np.random.randint(len(x_train))\n",
    "    img = x_train[idx].reshape(28, 28)\n",
    "    label = alphabet[y_train[idx]]\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    ax.set_title(f'{label}')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ecuaciones para nuestro modelo\n",
    "\n",
    "\n",
    "$$z^1 = W^1 X + b^1$$\n",
    "\n",
    "$$a^1 = ReLU(z^1) $$\n",
    "\n",
    "$$z^2 = W^2 a^1 + b^2$$\n",
    "\n",
    "$$\\hat{y} = \\frac{e^{z^{2_k}}}{\\sum_j{e^{z_j}}}$$\n",
    "\n",
    "\n",
    "$$ \\mathcal{L}(\\hat{y}^{i}, y^{i}) =  - y^{i}  \\ln(\\hat{y}^{i}) = -\\ln(\\hat{y}^i)$$\n",
    "\n",
    "\n",
    "$$ \\mathcal{J}(w, b) =  \\frac{1}{num\\_samples} \\sum_{i=1}^{num\\_samples}-\\ln(\\hat{y}^{i})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones adicionales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mini batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batches(x, y, batch_size=32, shuffle=True):\n",
    "    '''\n",
    "    Create mini batches from the dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    x: numpy array of features\n",
    "    y: numpy array of labels\n",
    "    batch_size: size of each mini batch (default 32)\n",
    "    shuffle: whether to shuffle the data before creating batches (default True)\n",
    "    \n",
    "    Returns:\n",
    "    List of tuples (x_batch, y_batch)\n",
    "    '''\n",
    "    \n",
    "    num_samples = len(x)\n",
    "    indices = np.arange(num_samples)\n",
    "    \n",
    "    # Shuffle if requested\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "    \n",
    "    # Create mini batches\n",
    "    mini_batches = []\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        batch_indices = indices[i:i + batch_size]\n",
    "        x_batch = x[batch_indices]\n",
    "        y_batch = y[batch_indices]\n",
    "        mini_batches.append((x_batch, y_batch))\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nuestra clase Linear, ReLU y Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Clase Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    '''\n",
    "    Linear (fully connected) layer implementation.\n",
    "    \n",
    "    Parameters:\n",
    "    input_size: number of input features\n",
    "    output_size: number of output features (neurons)\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, input_size, output_size):\n",
    "        '''\n",
    "        Initialize the linear layer with weights and biases.\n",
    "        \n",
    "        Weights are initialized with small random values (Xavier/He initialization).\n",
    "        Biases are initialized to zeros.\n",
    "        '''\n",
    "        # Xavier initialization for weights\n",
    "        limit = np.sqrt(6 / (input_size + output_size))\n",
    "        self.W = np.random.uniform(-limit, limit, size=(input_size, output_size)).astype(np.float32)\n",
    "        self.b = np.zeros((1, output_size)).astype(np.float32)\n",
    "        \n",
    "        # Store gradients for backpropagation\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "        # Store input for backpropagation\n",
    "        self.X = None\n",
    "    \n",
    "    def forward(self, X):\n",
    "        '''\n",
    "        Forward pass: compute Z = X @ W + b\n",
    "        \n",
    "        Parameters:\n",
    "        X: input data of shape (batch_size, input_size)\n",
    "        \n",
    "        Returns:\n",
    "        Z: output of shape (batch_size, output_size)\n",
    "        '''\n",
    "        self.X = X\n",
    "        Z = X @ self.W + self.b\n",
    "        return Z\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        '''\n",
    "        Backward pass: compute gradients with respect to W, b, and X\n",
    "        \n",
    "        Parameters:\n",
    "        dZ: gradient of loss with respect to Z, shape (batch_size, output_size)\n",
    "        \n",
    "        Returns:\n",
    "        dX: gradient of loss with respect to X, shape (batch_size, input_size)\n",
    "        '''\n",
    "        batch_size = self.X.shape[0]\n",
    "        \n",
    "        # Gradients with respect to weights and biases\n",
    "        self.dW = (self.X.T @ dZ) / batch_size\n",
    "        self.db = np.sum(dZ, axis=0, keepdims=True) / batch_size\n",
    "        \n",
    "        # Gradient with respect to input\n",
    "        dX = dZ @ self.W.T\n",
    "        \n",
    "        return dX\n",
    "    \n",
    "    def update_parameters(self, learning_rate):\n",
    "        '''\n",
    "        Update weights and biases using gradient descent.\n",
    "        \n",
    "        Parameters:\n",
    "        learning_rate: learning rate for the update\n",
    "        '''\n",
    "        self.W -= learning_rate * self.dW\n",
    "        self.b -= learning_rate * self.db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clase ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    '''\n",
    "    ReLU (Rectified Linear Unit) activation function implementation.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Initialize the ReLU layer.\n",
    "        Store input for backpropagation.\n",
    "        '''\n",
    "        self.Z = None\n",
    "    \n",
    "    def forward(self, Z):\n",
    "        '''\n",
    "        Forward pass: compute ReLU activation\n",
    "        ReLU(Z) = max(0, Z)\n",
    "        \n",
    "        Parameters:\n",
    "        Z: input data of shape (batch_size, num_features)\n",
    "        \n",
    "        Returns:\n",
    "        A: activated output of shape (batch_size, num_features)\n",
    "        '''\n",
    "        self.Z = Z\n",
    "        A = np.maximum(0, Z)\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        '''\n",
    "        Backward pass: compute gradient with respect to Z\n",
    "        \n",
    "        Parameters:\n",
    "        dA: gradient of loss with respect to A, shape (batch_size, num_features)\n",
    "        \n",
    "        Returns:\n",
    "        dZ: gradient of loss with respect to Z, shape (batch_size, num_features)\n",
    "        '''\n",
    "        # Gradient is 1 where Z > 0, and 0 where Z <= 0\n",
    "        dZ = dA * (self.Z > 0).astype(np.float32)\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clase Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential:\n",
    "    '''\n",
    "    Sequential model that stacks multiple layers.\n",
    "    Supports forward pass, backward pass, and parameter updates.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, layers):\n",
    "        '''\n",
    "        Initialize the Sequential model with a list of layers.\n",
    "        \n",
    "        Parameters:\n",
    "        layers: list of layer objects (Linear, ReLU, etc.)\n",
    "        '''\n",
    "        self.layers = layers\n",
    "    \n",
    "    def forward(self, X):\n",
    "        '''\n",
    "        Forward pass through all layers.\n",
    "        \n",
    "        Parameters:\n",
    "        X: input data of shape (batch_size, input_size)\n",
    "        \n",
    "        Returns:\n",
    "        A: output after passing through all layers\n",
    "        '''\n",
    "        A = X\n",
    "        for layer in self.layers:\n",
    "            A = layer.forward(A)\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        '''\n",
    "        Backward pass through all layers in reverse order.\n",
    "        \n",
    "        Parameters:\n",
    "        dZ: gradient of loss with respect to the output\n",
    "        \n",
    "        Returns:\n",
    "        dX: gradient of loss with respect to the input\n",
    "        '''\n",
    "        dA = dZ\n",
    "        for layer in reversed(self.layers):\n",
    "            dA = layer.backward(dA)\n",
    "        return dA\n",
    "    \n",
    "    def update_parameters(self, learning_rate):\n",
    "        '''\n",
    "        Update parameters for all layers that have learnable parameters.\n",
    "        \n",
    "        Parameters:\n",
    "        learning_rate: learning rate for the update\n",
    "        '''\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'update_parameters'):\n",
    "                layer.update_parameters(learning_rate)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Make predictions on input data.\n",
    "        Returns the class with the highest probability.\n",
    "        \n",
    "        Parameters:\n",
    "        X: input data of shape (num_samples, input_size)\n",
    "        \n",
    "        Returns:\n",
    "        predictions: array of predicted class indices\n",
    "        '''\n",
    "        output = self.forward(X)\n",
    "        return np.argmax(output, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    '''\n",
    "    Compute softmax activation for multi-class classification.\n",
    "    \n",
    "    Parameters:\n",
    "    Z: numpy array of shape (batch_size, num_classes)\n",
    "    \n",
    "    Returns:\n",
    "    A: softmax probabilities of shape (batch_size, num_classes)\n",
    "    '''\n",
    "    # Subtract max for numerical stability\n",
    "    Z_shifted = Z - np.max(Z, axis=1, keepdims=True)\n",
    "    exp_Z = np.exp(Z_shifted)\n",
    "    A = exp_Z / np.sum(exp_Z, axis=1, keepdims=True)\n",
    "    return A\n",
    "\n",
    "\n",
    "def cross_entropy_loss(y_pred, y_true):\n",
    "    '''\n",
    "    Compute cross-entropy loss for multi-class classification.\n",
    "    \n",
    "    Parameters:\n",
    "    y_pred: softmax probabilities of shape (batch_size, num_classes)\n",
    "    y_true: true labels (class indices) of shape (batch_size,)\n",
    "    \n",
    "    Returns:\n",
    "    loss: average cross-entropy loss across the batch\n",
    "    '''\n",
    "    batch_size = len(y_true)\n",
    "    # Get probabilities of true classes\n",
    "    correct_logprobs = -np.log(y_pred[np.arange(batch_size), y_true])\n",
    "    loss = np.mean(correct_logprobs)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def compute_loss_and_gradients(y_pred, y_true):\n",
    "    '''\n",
    "    Compute loss and gradients with respect to predictions.\n",
    "    \n",
    "    Parameters:\n",
    "    y_pred: softmax probabilities of shape (batch_size, num_classes)\n",
    "    y_true: true labels (class indices) of shape (batch_size,)\n",
    "    \n",
    "    Returns:\n",
    "    loss: cross-entropy loss\n",
    "    dZ: gradient of loss with respect to pre-softmax output\n",
    "    '''\n",
    "    batch_size = len(y_true)\n",
    "    loss = cross_entropy_loss(y_pred, y_true)\n",
    "    \n",
    "    # Gradient of cross-entropy with softmax\n",
    "    dZ = y_pred.copy()\n",
    "    dZ[np.arange(batch_size), y_true] -= 1\n",
    "    \n",
    "    return loss, dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with dynamic architecture\n",
    "def train_model(model, x_train, y_train, x_val, y_val, epochs=50, batch_size=32, learning_rate=0.01):\n",
    "    '''\n",
    "    Train the neural network model.\n",
    "    \n",
    "    Parameters:\n",
    "    model: Sequential model to train\n",
    "    x_train: training features\n",
    "    y_train: training labels\n",
    "    x_val: validation features\n",
    "    y_val: validation labels\n",
    "    epochs: number of training epochs\n",
    "    batch_size: size of mini batches\n",
    "    learning_rate: learning rate for gradient descent\n",
    "    \n",
    "    Returns:\n",
    "    history: dictionary with training metrics\n",
    "    '''\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    num_classes = len(np.unique(y_train))\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Create mini batches\n",
    "        mini_batches = create_mini_batches(x_train, y_train, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        epoch_loss = 0\n",
    "        epoch_correct = 0\n",
    "        epoch_total = 0\n",
    "        \n",
    "        # Training phase\n",
    "        for x_batch, y_batch in mini_batches:\n",
    "            # Forward pass\n",
    "            logits = model.forward(x_batch)\n",
    "            predictions = softmax(logits)\n",
    "            \n",
    "            # Compute loss and gradients\n",
    "            batch_loss, dZ = compute_loss_and_gradients(predictions, y_batch)\n",
    "            epoch_loss += batch_loss * len(y_batch)\n",
    "            \n",
    "            # Count correct predictions\n",
    "            batch_pred = np.argmax(predictions, axis=1)\n",
    "            epoch_correct += np.sum(batch_pred == y_batch)\n",
    "            epoch_total += len(y_batch)\n",
    "            \n",
    "            # Backward pass\n",
    "            model.backward(dZ)\n",
    "            \n",
    "            # Update parameters\n",
    "            model.update_parameters(learning_rate)\n",
    "        \n",
    "        # Calculate epoch metrics\n",
    "        train_loss = epoch_loss / epoch_total\n",
    "        train_acc = epoch_correct / epoch_total\n",
    "        \n",
    "        # Validation phase\n",
    "        val_logits = model.forward(x_val)\n",
    "        val_predictions = softmax(val_logits)\n",
    "        val_loss = cross_entropy_loss(val_predictions, y_val)\n",
    "        val_pred = np.argmax(val_predictions, axis=1)\n",
    "        val_acc = np.sum(val_pred == y_val) / len(y_val)\n",
    "        \n",
    "        # Store history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs} - Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create your model and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define model architecture dynamically\n",
    "input_size = x_train.shape[1]  # 784 for 28x28 images\n",
    "num_classes = len(alphabet)  # 24 classes\n",
    "\n",
    "# Create model with dynamic layers\n",
    "model = Sequential([\n",
    "    Linear(input_size, 128),\n",
    "    ReLU(),\n",
    "    Linear(128, 64),\n",
    "    ReLU(),\n",
    "    Linear(64, num_classes)\n",
    "])\n",
    "\n",
    "# Normalize validation and test data with same method\n",
    "x_val_new = normalise(x_val_new, method='scale255')\n",
    "x_test = normalise(x_test, method='scale255')\n",
    "\n",
    "# Train the model\n",
    "history = train_model(model, x_train, y_train, x_val_new, y_val_new, \n",
    "                      epochs=50, batch_size=32, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your model on Random data from your test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.randint(len(y_test))\n",
    "plot_number(x_test[idx].reshape(28,28))\n",
    "pred = model.predict(x_test[idx].reshape(-1, 1))\n",
    "print(f'el valor predicho es: {alphabet[pred]} el valor real es:{alphabet[y_test[idx]]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
